{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "Consider the bivariate function f : $R^2$ → R that is defined as follows: \n",
    "\\begin{equation*} f(\\mathbf{w}) = (w_1^2 + w_2-11)^2 +(w_1+w_2^2-7)^2 \\end{equation*}\n",
    "\n",
    "Provide an implementation of gradient descent that minimizes this function while adhering to the following\n",
    "requirements:\n",
    "   \n",
    "• You cannot use any optimization-related functions from numpy, scipy, etc. Your implementation must\n",
    "compute the gradient ∇f(w) numerically; in other words, you cannot analytically evaluate the gradient.\n",
    "• Your implementation should declare that it has found a minimum w∗ if (and when)  $||∇f(w^∗)||_2$ falls\n",
    "below $10^{−12}$. Your implementation should declare failure if it cannot find $w^∗$ within 10,000 iterations.\n",
    "1. Initialize gradient descent from $w_0 =[0 −4]^T$\n",
    ".\n",
    "\n",
    "    – Run the algorithm with step size $\\gamma$ = 0.005 and, if the algorithm converges, output $w^∗$ and\n",
    "the number of iterations it took the algorithm to converge.\n",
    "    \n",
    "    – Run the algorithm with step size $\\gamma$ = 0.01 and, if the algorithm converges, output $w^∗$ and the\n",
    "number of iterations it took the algorithm to converge.\n",
    "\n",
    "    – Comment on the convergence speed of gradient descent for this problem as a function of the\n",
    "step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run gradient descent with step size $\\gamma$ = 0.01 for four different initializations: (i) $w_0 =[0, −4]^T$;\n",
    "(ii) $w_0 = [0.5, −4]^T$; (iii) $w_0 = [0 , 4]^T$; and (iv) $w_0 = [0.5 ,4]^T$.\n",
    "\n",
    "    – Compare the solutions returned in each one of the four runs; are all the solutions the same?\n",
    "Based on the information available to you from these runs, are the solutions local minima or\n",
    "global minima?\n",
    "\n",
    "    – Generate a contour plot of f(w) over the region [−5, 5] × [−5, 5] using 100 contour lines and\n",
    "overlay on this plot the four gradient descent solution paths corresponding to the four different\n",
    "initializations. Refer to the following figure corresponding to $f(w) = 0.2w_1^1 + w_2^2$ and a single initialization of $w^0 = [4,4]^T$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first thing we need to do is to evaluate the gradient for our function\n",
    "# We are allowed to calculate the gradient using the numpy.diff function\n",
    "# The derivative would then be [diff(w1)/diff(y),diff(w2)/diff(y)] \n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import *\n",
    "w1 = array([[1,10],[2,20]])\n",
    "w2 = array([[2,5],[5,25]])\n",
    "eq = (w1**2+w2-11)**2+(w1+w2**2-7)**2\n",
    "V = [np.diff(w1)/np.diff(eq),np.diff(w2)/np.diff(eq)] #This is our derivative of our function, it is needed to be taken in respect to w1 and w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1\n",
    "# once we find the gradient, in order to do the gradient descent we need to initialize our tools\n",
    "w0 = np.transpose([0 , -4]) # this is what our descent starts at \n",
    "gamma1 = 0.005 #This is our learning rate/ step size\n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of iterations 1 \n",
      " w is  [ 0.   -3.98]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-baf4c743b8e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We then have to run the loop that will run our iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mprevious_gamma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcurrent_iteration\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprev_w0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mw0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_w0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This is the iterative gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprevious_gamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprev_w0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#This is the change in x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma1 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "\n",
    "print (\"Local minimum w* occurs at \", w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 Part 2 is the same thing just a different step size\n",
    "# Problem 1\n",
    "# once we find the gradient, in order to do the gradient descent we need to initialize our tools\n",
    "w0 = np.transpose([0  -4]) # this is what our descent starts at \n",
    "gamma2 = 0.01 #This is our second step size \n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n",
    "\n",
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma2 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "    \n",
    "print (\"Local minimum w* occurs at \", w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2 we are just using different w0 values\n",
    "#Part 1\n",
    "w0 = np.transpose([0  -4]) # this is what our descent starts at \n",
    "gamma2 = 0.01 #This is our second step size \n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n",
    "\n",
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma2 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "    \n",
    "print (\"Local minimum w* occurs at \", w0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2\n",
    "w0 = np.transpose([0.5 -4]) # this is what our descent starts at \n",
    "gamma2 = 0.01 #This is our second step size \n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n",
    "\n",
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma2 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "    \n",
    "print (\"Local minimum w* occurs at \", w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Problem 2 part 3\n",
    "w0 = np.transpose([0 , 4]) # this is what our descent starts at \n",
    "gamma2 = 0.01 #This is our second step size \n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n",
    "\n",
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma2 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "    \n",
    "print (\"Local minimum w* occurs at \", w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of iterations 1 \n",
      " w is  [0.495 3.96 ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8f960383032a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# We then have to run the loop that will run our iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mprevious_gamma\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcurrent_iteration\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mprev_w0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mw0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_w0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This is the iterative gradient descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "#Problem 2 Part 4\n",
    "w0 = np.transpose([0.5 , 4]) # this is what our descent starts at \n",
    "gamma2 = 0.01 #This is our second step size \n",
    "previous_gamma = 1\n",
    "precision = 0.000000000001 # our program will declare it has found our function when it falls within this range\n",
    "max_iterations = 10000 # This is the amount of iterations our program will run\n",
    "current_iteration = 0 # This is our counter that  will return the number of iterations until w* is found\n",
    "df  = lambda V: V # This is our gradient of the function\n",
    "\n",
    "# We then have to run the loop that will run our iterations\n",
    "while previous_gamma > precision and current_iteration < max_iterations:\n",
    "    prev_w0 = w0\n",
    "    w0 = w0 - gamma2 * df(prev_w0) # This is the iterative gradient descent\n",
    "    previous_gamma = abs(w0-prev_w0) #This is the change in x\n",
    "    current_iteration = current_iteration + 1 # increment our counter to be printed later\n",
    "    print (\"# of iterations\" , current_iteration, \"\\n w is \", w0)\n",
    "    \n",
    "print (\"Local minimum w* occurs at \", w0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
